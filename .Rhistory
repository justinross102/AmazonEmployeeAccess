WHERE (s.Form = d.Form) AND (s.Qnum = d.Qnum)
AND s.FORM = 'D'
ORDER BY StudentID")
result1 <- dbSendQuery(mydb, query1)
data1 <- fetch(result1, n=-1)
query2 <- paste("SELECT s.StudentID, SUM(Score) AS Score, SUM(Score)/150 AS Percentages
FROM Students s, Domain d
WHERE (s.Form = d.Form) AND (s.Qnum = d.Qnum)
AND s.FORM = 'D'
GROUP BY s.StudentID")
result2 <- dbSendQuery(mydb, query2)
data2 <- fetch(result2, n=-1)
data2
auto_grade <- function(table) {
table$Grade <- 0
for (i in 1:nrow(table)) {
if (table$Percentages[i] >= 0.90 & table$Percentages[i] <= 1.0) {
table$Grade[i] <- 'A'
} else if (table$Percentages[i] >= 0.80 & table$Percentages[i] < 0.90) {
table$Grade[i] <- 'B'
} else if (table$Percentages[i] >= 0.70 & table$Percentages[i] < 0.80) {
table$Grade[i] <- 'C'
} else if (table$Percentages[i] >= 0.60 & table$Percentages[i] < 0.70) {
table$Grade[i] <- 'D'
} else {
table$Grade[i] <- 'F'
}
}
return(table)
}
# add letter grade column via auto_grade function
data2 <- auto_grade(data2)
head(data2)
library(RMySQL)
mydb = dbConnect(MySQL(), user='root', password='Johnwilliams102', dbname='stats286_sqlfinal', host='localhost')
avg_grade = dbSendQuery(mydb,
"SELECT AVG(Percentages), STD(Percentages)
FROM Final_Results")
data = fetch(avg_grade, n=-1)
print(data)
mode_grade = dbSendQuery(mydb,
"SELECT Grade, Count(Grade)
FROM Final_Results
GROUP BY Grade
ORDER BY Count(Grade) DESC")
data = fetch(mode_grade, n=-1)
print(data)
# visualization
library(RMySQL)
library(tidyverse)
library(patchwork)
mydb <- dbConnect(MySQL(), user = 'root', password = 'Johnwilliams102',
dbname = 'stats286_sqlfinal', host = 'localhost')
result <- dbSendQuery(mydb, "select * from Final_Results")
data <- fetch(result, n=-1)
head(data)
plot1 <- data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
plot1
data$Grade <- factor(data$Grade, levels = c("A", "B", "C", "D", "F"))
plot1 <- data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
plot1
head(data)
data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
mydb <- dbConnect(MySQL(), user = 'root', password = 'Johnwilliams102',
dbname = 'stats286_sqlfinal', host = 'localhost')
result <- dbSendQuery(mydb, "select * from Final_Results")
data <- fetch(result, n=-1)
head(data)
data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
data$Grade <- factor(data$Grade, levels = c("A", "B", "C", "D", "F"))
data$Curved_Grade <- factor(data$Curved_Grade, levels = c("A", "B", "C", "D", "F"))
data$Grade <- factor(data$Grade, levels = c("A", "B", "C", "D", "F"))
data$Curved_Grade <- factor(data$Curved_Grade, levels = c("A", "B", "C", "D", "F"))
data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar()
data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
data %>%
ggplot(mapping = aes(Curved_Grade, fill = Curved_Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
plot1 + plot2
plot1 <- data %>%
ggplot(mapping = aes(Grade, fill = Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
plot2 <- data %>%
ggplot(mapping = aes(Curved_Grade, fill = Curved_Grade)) +
geom_bar() +
scale_x_discrete(drop = FALSE)  # This ensures that all levels are displayed
plot1 + plot2
(plot1 + plot2) / plo2
(plot1 + plot2) / plot2
(plot1 + plot2) / plot2 + plot1
(plot1 + plot2) / (plot2 + plot1)
both <- (plot1 + plot2)
(plot1 + plot2)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
# read in data ------------------------------------------------------------
setwd("/Users/justinross/Documents/BYU/stat348/ItemDemandChallenge")
train <- vroom("train.csv")
test <- vroom("test.csv")
library(modeltime) # Extensions of tidymodels to time series
library(timetk) # Some nice time series functions
storeItem1 <- train %>% filter(store==3, item==10)
storeItem2 <- train %>% filter(store==5, item==37)
cv_split1 <- time_series_split(storeItem1, assess="3 months", cumulative = TRUE)
cv_split2 <- time_series_split(storeItem2, assess="3 months", cumulative = TRUE)
cv_split1 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
cv_split2 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
storeItem1 <- train %>% filter(store==3, item==10)
storeItem2 <- train %>% filter(store==5, item==37)
cv_split1 <- time_series_split(storeItem1, assess="3 months", cumulative = TRUE)
cv_split2 <- time_series_split(storeItem2, assess="3 months", cumulative = TRUE)
cv_split1 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
cv_split2 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
storeItem1
my_recipe1 <- recipe(sales ~ ., storeItem1) %>%
step_date(date, features="month") %>%
step_date(date, features="year") %>%
step_date(date, features = "doy") %>%
step_date(date, features="dow") %>%
step_rm(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
my_recipe2 <- recipe(sales ~ ., storeItem2) %>%
step_date(date, features="month") %>%
step_date(date, features="year") %>%
step_date(date, features = "doy") %>%
step_date(date, features="dow") %>%
step_rm(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf1 <- workflow() %>%
add_model(arima_model) %>%
fit(sales~date, data=training(cv_split1))
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_model(arima_model) %>%
fit(sales~date, data=training(cv_split1))
arima_model <- arima_reg(seasonal_period=1,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_model(arima_model) %>%
fit(sales~date, data=training(cv_split1))
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_model(arima_model) %>%
fit(sales~date, data=training(cv_split1))
rlang::last_trace()
rlang::last_trace(drop = FALSE)
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
my_recipe1 <- recipe(sales ~ date, storeItem1) %>%
step_date(date, features="month") %>%
step_date(date, features="year") %>%
step_date(date, features = "doy") %>%
step_date(date, features="dow") %>%
step_rm(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
my_recipe2 <- recipe(sales ~ date, storeItem2) %>%
step_date(date, features="month") %>%
step_date(date, features="year") %>%
step_date(date, features = "doy") %>%
step_date(date, features="dow") %>%
step_rm(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
cv_split1
storeItem1
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
prepped <- prep(my_recipe1)
baked <- bake(prepped, data = training(cv_split1))
baked <- bake(prepped, data = storeItem1)
baked <- bake(prepped, data = training(cv_split1))
training(cv_split1)
split <- training(cv_split1)
baked <- bake(prepped, data = split)
split
baked <- bake(prepped, data = NULL)
prepped <- prep(my_recipe1)
baked <- bake(prepped, data = NULL)
baked <- bake(prepped, new_data = NULL)
baked
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf1 <- workflow() %>%
add_recipe(my_recipe1, training(cv_split1)) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf1 <- workflow() %>%
add_recipe(recipe(sales ~ date, training(cv_split1))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf2 <- workflow() %>%
add_recipe(recipe(sales ~ date, training(cv_split2))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split2))
## Cross-validate to tune model
cv_results1 <- modeltime_calibrate(arima_wf1,
new_data = testing(cv_split1))
cv_results2 <- modeltime_calibrate(arima_wf2,
new_data = testing(cv_split2))
# Refit to all data then forecast
es_fullfit1 <- cv_results1 %>%
modeltime_refit(data = storeItem1)
es_fullfit2 <- cv_results2 %>%
modeltime_refit(data = storeItem2)
# Refit to all data then forecast
arima_fullfit1 <- cv_results1 %>%
modeltime_refit(data = storeItem1)
arima_fullfit2 <- cv_results2 %>%
modeltime_refit(data = storeItem2)
arima_preds1 <- arima_fullfit1 %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
arima_preds2 <- arima_fullfit2 %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
## Visualize CV results
p1 <- cv_results1 %>%
modeltime_forecast(new_data = testing(cv_split1),
actual_data = storeItem1) %>%
plot_modeltime_forecast(.interactive = FALSE)
p2 <- cv_results2 %>%
modeltime_forecast(new_data = testing(cv_split2),
actual_data = storeItem2) %>%
plot_modeltime_forecast(.interactive = FALSE)
p1
p2
my_recipe1 <- recipe(sales ~ ., storeItem1) %>%
step_date(date, features="month") %>%
step_date(date, features="year") %>%
step_date(date, features = "doy") %>%
step_date(date, features="dow") %>%
step_rm(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
arima_wf1 <- workflow() %>%
add_recipe(recipe(my_recipe1, training(cv_split1))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf1 <- workflow() %>%
add_recipe(recipe(sales ~ date, training(cv_split1))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(modeltime) # Extensions of tidymodels to time series
library(timetk) # Some nice time series functions
# read in data ------------------------------------------------------------
setwd("/Users/justinross/Documents/BYU/stat348/ItemDemandChallenge")
train <- vroom("train.csv")
test <- vroom("test.csv")
storeItem1 <- train %>% filter(store==3, item==10)
storeItem2 <- train %>% filter(store==5, item==37)
cv_split1 <- time_series_split(storeItem1, assess="3 months", cumulative = TRUE)
cv_split2 <- time_series_split(storeItem2, assess="3 months", cumulative = TRUE)
cv_split1 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
cv_split2 %>%
tk_time_series_cv_plan() %>% # Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>%  #default max D to tune
set_engine("auto_arima")
arima_wf1 <- workflow() %>%
add_recipe(recipe(sales ~ date, training(cv_split1))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf2 <- workflow() %>%
add_recipe(recipe(sales ~ date, training(cv_split2))) %>%
add_model(arima_model) %>%
fit(data=training(cv_split2))
## Cross-validate to tune model
cv_results1 <- modeltime_calibrate(arima_wf1,
new_data = testing(cv_split1))
cv_results2 <- modeltime_calibrate(arima_wf2,
new_data = testing(cv_split2))
## Visualize CV results
p1 <- cv_results1 %>%
modeltime_forecast(new_data = testing(cv_split1),
actual_data = storeItem1) %>%
plot_modeltime_forecast(.interactive = FALSE)
p2 <- cv_results2 %>%
modeltime_forecast(new_data = testing(cv_split2),
actual_data = storeItem2) %>%
plot_modeltime_forecast(.interactive = FALSE)
p2
p1
# Refit to all data then forecast
arima_fullfit1 <- cv_results1 %>%
modeltime_refit(data = storeItem1)
arima_fullfit2 <- cv_results2 %>%
modeltime_refit(data = storeItem2)
arima_preds1 <- arima_fullfit1 %>%
modeltime_forecast(h = "3 months", new_data = storeItem1) %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
arima_preds2 <- arima_fullfit2 %>%
modeltime_forecast(h = "3 months", new_data = storeItem2) %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
p3 <- arima_fullfit1 %>%
modeltime_forecast(h = "3 months", actual_data = storeItem1) %>%
plot_modeltime_forecast(.interactive=FALSE)
p4 <- arima_fullfit2 %>%
modeltime_forecast(h = "3 months", actual_data = storeItem2) %>%
plot_modeltime_forecast(.interactive=FALSE)
p3
p4
submission <- plotly::subplot(p1,p2,p3,p4, nrows = 2)
submission
# load libraries
suppressMessages(library(tidyverse))
suppressMessages(library(tidymodels))
suppressMessages(library(vroom))
suppressMessages(library(corrplot))
suppressMessages(library(discrim)) # naive bayes
suppressMessages(library(embed)) # for target encoding
suppressMessages(library(themis)) # for balancing
train <- vroom("training.csv")
# load in data ------------------------------------------------------------
setwd("/Users/justinross/Documents/BYU/stat348/DontGetKicked")
train <- vroom("training.csv")
train[train == "NULL"] <- NA
test <- vroom("/kaggle/input/DontGetKicked/test.zip")
test <- vroom("test.csv")
test[test == "NULL"] <- NA
# just to retain original ID numbers
test2 <- vroom("test.csv")
# load libraries ----------------------------------------------------------
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(discrim)
library(naivebayes)
library(doParallel)
library(kknn)
library(kernlab)
library(themis)
library(stacks)
# load in data ------------------------------------------------------------
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
predict_and_format <- function(workflow, new_data, filename){
predictions <- workflow %>%
predict(new_data = new_data,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
vroom_write(x = submission, file = filename, delim=",")
}
predict_and_format <- function(workflow, new_data, filename){
predictions <- workflow %>%
predict(new_data = new_data,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1")# %>%
#select(3,2)
vroom_write(x = submission, file = filename, delim=",")
}
recipe1 <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%  # combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors())  # dummy variable encoding
setwd("/Users/justinross/Documents/BYU/stat348/AmazonEmployeeAccess")
# load in data ------------------------------------------------------------
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
predict_and_format <- function(workflow, new_data, filename){
predictions <- workflow %>%
predict(new_data = new_data,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1")# %>%
#select(3,2)
vroom_write(x = submission, file = filename, delim=",")
}
recipe1 <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%  # combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors())  # dummy variable encoding
balance_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) %>% #Everything numeric for SMOTE
step_downsample(all_outcomes())
logistic_mod <- logistic_reg() %>%
set_engine("glm")
logistic_workflow <- workflow() %>%
add_recipe(balance_recipe) %>%
add_model(logistic_mod) %>%
fit(data = train) # Fit the workflow
predict_and_format(logistic_workflow, test, "./logistic_predictions.csv")
