merged_data
all_games <- bind_cols(phillies_games, merged_data)
view(all_games)
all_games
# predictions -------------------------------------------------------------
library(tidymodels)
library(embed) # for target encoding
# Generate random indices for train and test sets
indices <- sample(1:nrow(all_games), size = 0.7 * nrow(all_games))
# Create training and testing sets
train <- all_games[indices, ] %>%
mutate(Outcome = as.factor(Outcome))
train
test <- all_games[-indices, ] %>%
select(-Outcome)
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%  # combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors())  # dummy variable encoding
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# model
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%  # combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors())  # dummy variable encoding
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
show_notes(.Last.tune.result)
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
step_dummy(all_nominal_predictors())  # dummy variable encoding
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
show_notes(.Last.tune.result)
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
# model
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
train
# Generate random indices for train and test sets
indices <- sample(1:nrow(all_games), size = 0.7 * nrow(all_games))
# Create training and testing sets
train <- all_games[indices, ] %>%
mutate(Outcome = as.factor(Outcome))
test <- all_games[-indices, ] %>%
select(-Outcome)
# model
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) %>% # turn all nominal features into factors
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# Generate random indices for train and test sets
indices <- sample(1:nrow(all_games), size = 0.7 * nrow(all_games))
# Create training and testing sets
train <- all_games[indices, ] %>%
mutate(Outcome = as.factor(Outcome))
test <- all_games[-indices, ] %>%
select(-Outcome)
# model
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
my_recipe <- recipe(Outcome ~ ., data = train) %>%
step_date(Date, features="dow") %>% # pull out individual variables from datetime
step_date(Date, features="month") %>%
step_rm(Date) %>% # don't need it anymore
step_mutate_at(all_nominal_predictors(), fn = factor) # turn all nominal features into factors
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
prepped <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data = train) # should have 112 columns
baked <- bake(prepped, new_data = train) # should have 112 columns
baked
view(baked)
target_encoding_recipe <- recipe(Outcome ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%  # combines categorical values that occur <1% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # target encoding (must be 2-factor)
prepped <- prep(target_encoding_recipe)
target_encoding_recipe <- recipe(Outcome ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%  # combines categorical values that occur <1% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(Outcome)) # target encoding (must be 2-factor)
prepped <- prep(target_encoding_recipe)
baked <- bake(prepped, new_data = train) # should have 112 columns
view(baked)
rand_forest_workflow <- workflow() %>%
add_recipe(target_encoding_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
## Find Best Tuning Parameters
pen_bestTune <- CV_results %>%
select_best("roc_auc")
pen_bestTune
## Find Best Tuning Parameters
pen_bestTune <- CV_results %>%
select_best("roc_auc")
## Finalize the Workflow & fit it
final_pen_wf <- rand_forest_workflow %>%
finalize_workflow(pen_bestTune) %>%
fit(data = train)
predictions <- final_pen_wf %>%
predict(new_data = test,
type = "prob")
predictions
matching_rows <- all_games[all_games$Date %in% test$Date, ]
# ROC curve and AUC for the training set
roc_train <- roc(y_train, pred_train)
view(test)
matching_rows <- all_games[all_games$Date %in% test$Date, ]
view(matching_rows)
matching_rows <- matching_rows[-2, ]
view(matching_rows)
matching_rows <- matching_rows[-43, ]
view(matching_rows)
install.packages("pROC")
library(pROC)
# ROC curve and AUC for the training set
roc_train <- roc(train$Outcome, predictions$.pred_W)
# ROC curve and AUC for the training set
roc_train <- roc(test$Outcome, predictions$.pred_W)
matching_rows <- matching_rows[-43, ] %>%
mutate(Outcome = as.factor(Outcome))
# find out how good it is
matching_rows <- all_games[all_games$Date %in% test$Date, ]
matching_rows <- matching_rows[-2, ]
matching_rows <- matching_rows[-43, ] %>%
mutate(Outcome = as.factor(Outcome))
# ROC curve and AUC for the training set
roc_train <- roc(matching_rows$Outcome, predictions$.pred_W)
auc_train <- auc(roc_train)
# Print AUC values
cat("Training AUC:", auc_train, "\n")
cat("Testing AUC:", auc_test, "\n")
# Plot ROC curves
plot(roc_train, main = "ROC Curve - Training Set", col = "blue")
target_encoding_recipe <- recipe(Outcome ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%  # combines categorical values that occur <1% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(Outcome)) %>%  # target encoding (must be 2-factor)
step_nzv(all_nominal_predictors())
prepped <- prep(target_encoding_recipe)
baked <- bake(prepped, new_data = train) # should have 112 columns
view(baked)
library(tidyverse)
library(rvest)
get_html_table <- function(url, index, header = T){
df <- url %>%
read_html() %>%
html_elements("table") %>%
html_table(header=header) %>%
.[[index]]
df
}
url_defense <- "https://www.baseball-reference.com/teams/PHI/2023-lineups.shtml"
defense <- get_html_table(url_defense,1)
colnames(defense) <- c("Opp", "PHI_C", "PHI_First_B", "PHI_Second_B", "PHI_Third_B", "PHI_SS", "PHI_LF",
"PHI_CF", "PHI_RF", "PHI_P", "PHI_DH")
# clean
defense <- defense %>%
mutate(Date = as.Date(str_extract(Opp, "\\w{3},\\d{1,2}/\\d{1,2}"), format = "%a,%m/%d"),
Opp = str_extract(Opp, "(?<=\\sat\\s|\\svs\\s)[A-Z]+")) %>%
select(c(12,1,2,3,4,5,6,7,8,9,10,11)) %>%
head(162) %>%
select(-c(1,2))
url_batting <- "https://www.baseball-reference.com/teams/PHI/2023-batting-orders.shtml"
batting <- get_html_table(url_batting,1)
colnames(batting) <- c("Opp", "PHI_First", "PHI_Second", "PHI_Third", "PHI_Fourth",
"PHI_Fifth", "PHI_Sixth", "PHI_Seventh", "PHI_Eigth", "PHI_Ninth")
# clean
batting <- batting %>%
mutate(Date = as.Date(str_extract(Opp, "\\w{3},\\d{1,2}/\\d{1,2}"), format = "%a,%m/%d"),
Opp = str_extract(Opp, "(?<=\\sat\\s|\\svs\\s)[A-Z]+")) %>%
select(c(11,1,2,3,4,5,6,7,8,9,10)) %>%
head(162) %>%
select(-c(1,2))
for (col in names(batting)) { # remove - from player names
batting[[col]] <- gsub("-.*", "", batting[[col]])
}
website = "https://www.baseball-reference.com/teams/PHI/2023-schedule-scores.shtml"
phillies_games <- get_html_table(website, 1)
# clean
colnames(phillies_games) <- c("Game", "Date", "boxscore", "Team", "Location", "Opp", "Outcome", "R", "RA", "Inn", "W_L", "Rank",
"GB", "Win", "Loss", "Save", "Time", "D_N", "Attendance", "cLI", "Streak", "Orig_Scheduled")
phillies_games <- as_tibble(lapply(phillies_games, function(Outcome) sub("-.*", "", Outcome)))
phillies_games <- phillies_games %>%
filter(Game != "Gm#", # get rid of monthly headers
boxscore != "preview") %>%  # get rid of games that haven't been played yet
mutate(Location = if_else(Location == "@", "Away", "Home"),
D_N = if_else(D_N == "D", "Day", "Night"),
Date = as.Date(Date, format = "%A, %b %d")) %>%
select(c(2,5,18,6,7))
merged_data <- bind_cols(defense, batting)
all_games <- bind_cols(phillies_games, merged_data)
view(all_games)
colnames(all_games)
split_index <- initial_split(all_games, prop = 0.8)
# predictions -------------------------------------------------------------
library(tidymodels)
library(embed) # for target encoding
split_index <- initial_split(all_games, prop = 0.8)
train_data <- training(split_index) %>%
mutate(Outcome = as.factor(Outcome))
split_index <- initial_split(all_games, prop = 0.8)
train <- training(split_index) %>%
mutate(Outcome = as.factor(Outcome))
test_ <- testing(split_index) %>%
select(-Outcome)
# Create a recipe
outcome_var <- "Outcome"
rf_recipe <- recipe(formula = paste(outcome_var, "~ ."), data = train) %>%
step_date(Date) %>%
step_dummy(all_nominal(), -all_outcomes())
split_index <- initial_split(all_games, prop = 0.8)
train <- training(split_index) %>%
mutate(Outcome = as.factor(Outcome))
test_ <- testing(split_index) %>%
select(-Outcome)
# Create a recipe
outcome_var <- "Outcome"
rf_recipe <- recipe(formula = paste(outcome_var, "~ ."), data = train) %>%
step_date(Date) %>%
step_dummy(all_nominal(), -all_outcomes())
rf_recipe <- recipe(formula = as.formula(paste(outcome_var, "~ .")), data = train_data) %>%
step_date(Date) %>%
step_dummy(all_nominal(), -all_outcomes())
# model
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
prepped <- prep(target_encoding_recipe)
prepped <- prep(rf_recipe)
baked <- bake(prepped, new_data = train) # should have 112 columns
view(baked)
rand_forest_workflow <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rand_forest_mod)
# tuning grid
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
show_notes(.Last.tune.result)
library(tidyverse)
library(nycflights13)
flights
dat <- flights
dat$year <- 'Justin'
dat
setwd("~/Documents/BYU/stat348/AmazonEmployeeAccess")
# load libraries ----------------------------------------------------------
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(discrim)
library(naivebayes)
library(doParallel)
library(kknn)
library(kernlab)
library(themis)
# load in data ------------------------------------------------------------
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
predict_and_format <- function(workflow, new_data, filename){
predictions <- workflow %>%
predict(new_data = new_data,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
vroom_write(x = submission, file = filename, delim=",")
}
library(stacks)
folds <- vfold_cv(trainDataSet, v = 5, repeats=2)
untunedModel <- control_stack_grid()
folds <- vfold_cv(train, v = 5, repeats=2)
# create models to stack
penalized_logistic_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
penalized_reg_recipe <- recipe(ACTION ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # target encoding (must be 2-factor)
penalized_logistic_workflow <- workflow() %>%
add_recipe(balance_recipe_2) %>%
add_model(penalized_logistic_mod)
# load in data ------------------------------------------------------------
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
predict_and_format <- function(workflow, new_data, filename){
predictions <- workflow %>%
predict(new_data = new_data,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
vroom_write(x = submission, file = filename, delim=",")
}
penalized_logistic_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>% #Type of model
set_engine("glmnet")
# because of the penalty, this regression can handle categories with only a few observations
# so I removed the step_other()
penalized_reg_recipe <- recipe(ACTION ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # target encoding (must be 2-factor)
penalized_logistic_workflow <- workflow() %>%
add_recipe(penalized_reg_recipe) %>%
add_model(penalized_logistic_mod)
## Grid of values to tune over
pen_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
## Grid of values to tune over
pen_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
pen_folds <- vfold_cv(train, v = 5, repeats = 3)
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees = 1000) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
target_encoding_recipe <- recipe(ACTION ~ ., train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%  # combines categorical values that occur <1% into an "other" value
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # target encoding (must be 2-factor)
rand_forest_workflow <- workflow() %>%
add_recipe(target_encoding_recipe) %>%
add_model(rand_forest_mod)
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 2)
folds <- vfold_cv(train, v = 5, repeats=2)
untunedModel <- control_stack_grid()
preg_models <- preg_wf %>%
tune_grid(resamples=pen_folds,
grid=pen_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
preg_models <- penalized_logistic_workflow %>%
tune_grid(resamples=pen_folds,
grid=pen_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
randforest_models <- rand_forest_workflow %>%
tune_grid(resamples=forest_folds,
grid=rand_forest_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
randforest_models <- rand_forest_workflow %>%
tune_grid(resamples=forest_folds,
grid=rand_forest_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
# Specify with models to include
my_stack <- stacks() %>%
add_candidates(preg_models) %>%
add_candidates(randforest_models)
## Fit the stacked model
stack_mod <- my_stack %>%
blend_predictions() %>% # LASSO penalized regression meta-learner
fit_members() ## Fit the members to the dataset
folds <- vfold_cv(train, v = 5, repeats=2)
untunedModel <- control_stack_grid()
preg_models <- penalized_logistic_workflow %>%
tune_grid(resamples=folds,
grid=pen_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
randforest_models <- rand_forest_workflow %>%
tune_grid(resamples=folds,
grid=rand_forest_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
# Specify with models to include
my_stack <- stacks() %>%
add_candidates(preg_models) %>%
add_candidates(randforest_models)
## Fit the stacked model
stack_mod <- my_stack %>%
blend_predictions() %>% # LASSO penalized regression meta-learner
fit_members() ## Fit the members to the dataset
## Use the stacked data to get a prediction
preds <- stack_mod %>% predict(new_data=my_new_data)
## Use the stacked data to get a prediction
preds <- stack_mod %>% predict(new_data=test)
## Use the stacked data to get a prediction
preds <- stack_mod %>% predict(new_data=test)
preds
predictions <- stack_mod %>%
predict(new_data = test,
type = "prob")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
submission
vroom_write(x = submission, file = "stacked_predictions.csv", delim=",")
